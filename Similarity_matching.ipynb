{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMILARITY MATCHING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to simulate similarity matching using commonly used algorithms for text similarity scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries used (install using pip)\n",
    "# !pip install faker\n",
    "# !pip install python-Levenshtein\n",
    "# !pip install fuzzywuzzy\n",
    "# !pip install pandas\n",
    "# !pip install faker\n",
    "# !pip install scikit-learn\n",
    "# !pip install scipy\n",
    "# !pip install sentence-transformers\n",
    "# !pip install torch\n",
    "# !pip install python-Levenshtein[speedup]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "def generate_text_pairs():\n",
    "    fake = Faker()\n",
    "    texts = []\n",
    "    \n",
    "    # Base templates\n",
    "    tech_templates = [\n",
    "        \"The evolution of artificial intelligence has transformed {industry}. Recent advances in {tech_type} have enabled {capability}, leading to {benefit}. Companies like {company} are investing heavily in AI research, focusing on {specific_area}. This technology promises to {future_impact}, though challenges remain in {challenge_area}.\",\n",
    "        \"Machine learning applications in {industry} continue to expand. Particularly, {tech_type} has shown promising results in {capability}. Industry leaders such as {company} are developing solutions for {specific_area}, aiming to {future_impact}.\"\n",
    "    ]\n",
    "    \n",
    "    business_templates = [\n",
    "        \"Market analysis reveals significant growth in {sector} during {quarter}. Companies reported {percentage}% increase in revenue, driven by {factor}. Experts predict {prediction} by {year}, with {trend} emerging as a key trend.\",\n",
    "        \"The {sector} industry showed remarkable performance in {quarter}, with leading firms experiencing {percentage}% growth. {factor} played a crucial role in this expansion, while analysts forecast {prediction}.\"\n",
    "    ]\n",
    "    \n",
    "    # Generate 100 pairs\n",
    "    for i in range(100):\n",
    "        similarity = random.uniform(0, 1)\n",
    "        \n",
    "        if similarity > 0.8:  # High similarity\n",
    "            template = random.choice(tech_templates)\n",
    "            text1 = template.format(\n",
    "                industry=\"healthcare\",\n",
    "                tech_type=\"deep learning\",\n",
    "                capability=\"disease diagnosis\",\n",
    "                benefit=\"improved patient outcomes\",\n",
    "                company=\"DeepMind\",\n",
    "                specific_area=\"medical imaging\",\n",
    "                future_impact=\"revolutionize healthcare delivery\",\n",
    "                challenge_area=\"data privacy\"\n",
    "            )\n",
    "            # Slight variations for high similarity\n",
    "            text2 = template.format(\n",
    "                industry=\"healthcare\",\n",
    "                tech_type=\"deep learning\",\n",
    "                capability=\"disease detection\",\n",
    "                benefit=\"better patient outcomes\",\n",
    "                company=\"DeepMind\",\n",
    "                specific_area=\"medical imaging analysis\",\n",
    "                future_impact=\"transform healthcare delivery\",\n",
    "                challenge_area=\"data security\"\n",
    "            )\n",
    "        \n",
    "        elif 0.4 <= similarity <= 0.7:  # Medium similarity\n",
    "            text1 = fake.text(max_nb_chars=300)\n",
    "            text2 = text1[:150] + fake.text(max_nb_chars=150)\n",
    "        \n",
    "        else:  # Low similarity\n",
    "            text1 = fake.text(max_nb_chars=300)\n",
    "            text2 = fake.text(max_nb_chars=300)\n",
    "        \n",
    "        texts.append({\n",
    "            'id': i+1,\n",
    "            'text1': text1,\n",
    "            'text2': text2,\n",
    "            'similarity_score': round(similarity, 3)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(texts)\n",
    "\n",
    "# # Generate and save dataset\n",
    "df = generate_text_pairs()\n",
    "# df.to_csv('verbatim_text_data_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Speech guess able everything suddenly clearly....</td>\n",
       "      <td>Speech guess able everything suddenly clearly....</td>\n",
       "      <td>0.417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The evolution of artificial intelligence has t...</td>\n",
       "      <td>The evolution of artificial intelligence has t...</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Lose song pattern fear show produce keep. Refl...</td>\n",
       "      <td>Seven record agency hotel get. Office maintain...</td>\n",
       "      <td>0.712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Least west ok whether. Why sell lot your troub...</td>\n",
       "      <td>Least west ok whether. Why sell lot your troub...</td>\n",
       "      <td>0.563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Bar require prepare then gun they discover. Tr...</td>\n",
       "      <td>Matter audience production. Go if clear medica...</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Free little begin need including. Drug souther...</td>\n",
       "      <td>Treatment provide kid support. Forward later i...</td>\n",
       "      <td>0.186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>The evolution of artificial intelligence has t...</td>\n",
       "      <td>The evolution of artificial intelligence has t...</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Machine learning applications in healthcare co...</td>\n",
       "      <td>Machine learning applications in healthcare co...</td>\n",
       "      <td>0.907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Catch always hundred treat stand last. At choo...</td>\n",
       "      <td>Oil life administration.\\nSuccess development ...</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>Pattern face beyond cell network. Agent high t...</td>\n",
       "      <td>Huge south probably recognize feel peace. Toda...</td>\n",
       "      <td>0.712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              text1  \\\n",
       "0     1  Speech guess able everything suddenly clearly....   \n",
       "1     2  The evolution of artificial intelligence has t...   \n",
       "2     3  Lose song pattern fear show produce keep. Refl...   \n",
       "3     4  Least west ok whether. Why sell lot your troub...   \n",
       "4     5  Bar require prepare then gun they discover. Tr...   \n",
       "..  ...                                                ...   \n",
       "95   96  Free little begin need including. Drug souther...   \n",
       "96   97  The evolution of artificial intelligence has t...   \n",
       "97   98  Machine learning applications in healthcare co...   \n",
       "98   99  Catch always hundred treat stand last. At choo...   \n",
       "99  100  Pattern face beyond cell network. Agent high t...   \n",
       "\n",
       "                                                text2  similarity_score  \n",
       "0   Speech guess able everything suddenly clearly....             0.417  \n",
       "1   The evolution of artificial intelligence has t...             0.938  \n",
       "2   Seven record agency hotel get. Office maintain...             0.712  \n",
       "3   Least west ok whether. Why sell lot your troub...             0.563  \n",
       "4   Matter audience production. Go if clear medica...             0.167  \n",
       "..                                                ...               ...  \n",
       "95  Treatment provide kid support. Forward later i...             0.186  \n",
       "96  The evolution of artificial intelligence has t...             0.830  \n",
       "97  Machine learning applications in healthcare co...             0.907  \n",
       "98  Oil life administration.\\nSuccess development ...             0.091  \n",
       "99  Huge south probably recognize feel peace. Toda...             0.712  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Algorithms\n",
    "\n",
    "Here are the key text matching algorithms you can explore to kickstart your journey with similarity matching\n",
    "1. **String based similarity** - This matching algorithms is good to match words based on its spelling, it is used to identify or correct spelling mistakes etc.\n",
    "2. **Token based similarity** - This matching creates tokens for words, and matches based on frequency counts of the most common words. \n",
    "3. **Semantic Similarity** - considers the semantic meaning of the words, parts of speech and position of words in the sentence which allows a better matching context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String Based Similarity\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "from Levenshtein import distance\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def string_similarity(text1, text2):\n",
    "    # Levenshtein Distance\n",
    "    lev_ratio = 1 - distance(text1, text2) / max(len(text1), len(text2))\n",
    "    \n",
    "    # Sequence Matcher\n",
    "    seq_ratio = SequenceMatcher(None, text1, text2).ratio()\n",
    "    \n",
    "    # Fuzzy String Matching\n",
    "    fuzzy_ratio = fuzz.ratio(text1, text2) / 100\n",
    "    \n",
    "    return lev_ratio, seq_ratio, fuzzy_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token-Based Simliarity\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def token_similarity(text1, text2):\n",
    "    vectorizer = CountVectorizer().fit_transform([text1, text2])\n",
    "    vectors = vectorizer.toarray()\n",
    "    \n",
    "    # Jaccard Similarity\n",
    "    intersection = len(set(text1.split()) & set(text2.split()))\n",
    "    union = len(set(text1.split()) | set(text2.split()))\n",
    "    jaccard = intersection / union\n",
    "    \n",
    "    # Cosine Similarity with BOW\n",
    "    cosine_sim = 1 - cosine(vectors[0], vectors[1])\n",
    "    \n",
    "    return jaccard, cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(text1, text2):\n",
    "    # Convert texts to sets of words\n",
    "    set1 = set(text1.split())\n",
    "    set2 = set(text2.split())\n",
    "    \n",
    "    # Calculate intersection and union\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    \n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Based Similarity\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def semantic_similarity(text1, text2):\n",
    "    # Using BERT-based model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Generate embeddings\n",
    "    emb1 = model.encode(text1)\n",
    "    emb2 = model.encode(text2)\n",
    "    \n",
    "    # Compute similarity\n",
    "    cosine_sim = F.cosine_similarity(\n",
    "        torch.tensor(emb1).unsqueeze(0),\n",
    "        torch.tensor(emb2).unsqueeze(0)\n",
    "    )\n",
    "    \n",
    "    return cosine_sim.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "def bow_cosine_similarity(text1, text2):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectors = vectorizer.fit_transform([text1, text2])\n",
    "    dense = vectors.toarray()\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    dot_product = np.dot(dense[0], dense[1])\n",
    "    norm1 = np.linalg.norm(dense[0])\n",
    "    norm2 = np.linalg.norm(dense[1])\n",
    "    \n",
    "    return dot_product / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_cosine_similarity(text1, text2):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform([text1, text2])\n",
    "    dense = vectors.toarray()\n",
    "    \n",
    "    dot_product = np.dot(dense[0], dense[1])\n",
    "    norm1 = np.linalg.norm(dense[0])\n",
    "    norm2 = np.linalg.norm(dense[1])\n",
    "    \n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "# Function (similar approach above but using a simpler implementation - use either one)\n",
    "def tfidf_similarity(doc1, doc2):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([doc1, doc2])\n",
    "    return 1 - cosine(tfidf_matrix.toarray()[0], tfidf_matrix.toarray()[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard: 0.7142857142857143\n",
      "BOW: 0.8749999999999998\n",
      "TF-IDF: 0.7799154245579976\n",
      "TF-IDF: 0.7799154245579976\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog sat on the mat\",\n",
    "    \"The weather is nice today\"\n",
    "]\n",
    "\n",
    "print(f\"Jaccard: {jaccard_similarity(texts[0], texts[1])}\")      # Higher\n",
    "print(f\"BOW: {bow_cosine_similarity(texts[0], texts[1])}\")       # Higher\n",
    "print(f\"TF-IDF: {tfidf_cosine_similarity(texts[0], texts[1])}\")  # Lower (common words penalized)\n",
    "print(f\"TF-IDF: {tfidf_similarity(texts[0], texts[1])}\")  # Lower (common words penalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Jaccard'] = df.apply(lambda x: jaccard_similarity(x['text1'], x['text2']), axis=1)\n",
    "df['bow_cosine_similarity'] = df.apply(lambda x: bow_cosine_similarity(x['text1'], x['text2']), axis=1)\n",
    "df['tfidf_cosine_similarity'] = df.apply(lambda x: tfidf_cosine_similarity(x['text1'], x['text2']), axis=1)\n",
    "df['semantic_similarity'] = df.apply(lambda x: semantic_similarity(x['text1'], x['text2']), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Example using the Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>tfidf_cosine_similarity</th>\n",
       "      <th>Jaccard</th>\n",
       "      <th>bow_cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Speech guess able everything suddenly clearly....</td>\n",
       "      <td>Speech guess able everything suddenly clearly....</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.393520</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.557364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The evolution of artificial intelligence has t...</td>\n",
       "      <td>The evolution of artificial intelligence has t...</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.858762</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.923111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Lose song pattern fear show produce keep. Refl...</td>\n",
       "      <td>Seven record agency hotel get. Office maintain...</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.047622</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.089893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Least west ok whether. Why sell lot your troub...</td>\n",
       "      <td>Least west ok whether. Why sell lot your troub...</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.473771</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.640057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Bar require prepare then gun they discover. Tr...</td>\n",
       "      <td>Matter audience production. Go if clear medica...</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.011135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Free little begin need including. Drug souther...</td>\n",
       "      <td>Treatment provide kid support. Forward later i...</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>The evolution of artificial intelligence has t...</td>\n",
       "      <td>The evolution of artificial intelligence has t...</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.858762</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.923111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Machine learning applications in healthcare co...</td>\n",
       "      <td>Machine learning applications in healthcare co...</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.890324</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.941242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Catch always hundred treat stand last. At choo...</td>\n",
       "      <td>Oil life administration.\\nSuccess development ...</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.025783</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.049507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>Pattern face beyond cell network. Agent high t...</td>\n",
       "      <td>Huge south probably recognize feel peace. Toda...</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.012980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              text1  \\\n",
       "0     1  Speech guess able everything suddenly clearly....   \n",
       "1     2  The evolution of artificial intelligence has t...   \n",
       "2     3  Lose song pattern fear show produce keep. Refl...   \n",
       "3     4  Least west ok whether. Why sell lot your troub...   \n",
       "4     5  Bar require prepare then gun they discover. Tr...   \n",
       "..  ...                                                ...   \n",
       "95   96  Free little begin need including. Drug souther...   \n",
       "96   97  The evolution of artificial intelligence has t...   \n",
       "97   98  Machine learning applications in healthcare co...   \n",
       "98   99  Catch always hundred treat stand last. At choo...   \n",
       "99  100  Pattern face beyond cell network. Agent high t...   \n",
       "\n",
       "                                                text2  similarity_score  \\\n",
       "0   Speech guess able everything suddenly clearly....             0.417   \n",
       "1   The evolution of artificial intelligence has t...             0.938   \n",
       "2   Seven record agency hotel get. Office maintain...             0.712   \n",
       "3   Least west ok whether. Why sell lot your troub...             0.563   \n",
       "4   Matter audience production. Go if clear medica...             0.167   \n",
       "..                                                ...               ...   \n",
       "95  Treatment provide kid support. Forward later i...             0.186   \n",
       "96  The evolution of artificial intelligence has t...             0.830   \n",
       "97  Machine learning applications in healthcare co...             0.907   \n",
       "98  Oil life administration.\\nSuccess development ...             0.091   \n",
       "99  Huge south probably recognize feel peace. Toda...             0.712   \n",
       "\n",
       "    tfidf_cosine_similarity   Jaccard  bow_cosine_similarity  \n",
       "0                  0.393520  0.375000               0.557364  \n",
       "1                  0.858762  0.784314               0.923111  \n",
       "2                  0.047622  0.012048               0.089893  \n",
       "3                  0.473771  0.470588               0.640057  \n",
       "4                  0.011135  0.000000               0.021760  \n",
       "..                      ...       ...                    ...  \n",
       "95                 0.000000  0.000000               0.000000  \n",
       "96                 0.858762  0.784314               0.923111  \n",
       "97                 0.890324  0.794118               0.941242  \n",
       "98                 0.025783  0.012658               0.049507  \n",
       "99                 0.012980  0.000000               0.025318  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python connecting to excel document (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to excel file:\n",
    "def read_and_append_excel(file_path, sheet_name='Sheet1'):\n",
    "    # Read the existing excel file\n",
    "    existing_df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    \n",
    "    # Append the new dataframe (df) to the existing dataframe\n",
    "    combined_df = pd.concat([existing_df, df], axis=1)\n",
    "    \n",
    "    # Write the combined dataframe back to the excel file\n",
    "    with pd.ExcelWriter(file_path, engine='openpyxl', mode='a') as writer:\n",
    "        combined_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    # Example usage\n",
    "    # read_and_append_excel('path_to_your_excel_file.xlsx')\n",
    "    # Required libraries for reading and writing Excel files\n",
    "    # !pip install openpyxl\n",
    "    # !pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative approach is to use a pre-trained language model i.e. Spacy\n",
    "Spacy is a pre-trained statistical model using word vectors.\n",
    "in short, its similar to a language model and has been pre-trained to identify text,its parts of speech(POS) adding semantic meaning to the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another approach is using spacy\n",
    "# this is a pretrained model which calculates similarity based on two texts \n",
    "# reference - https://www.youtube.com/watch?v=DIxxz_DvqLA&t=301s\n",
    "\n",
    "import spacy\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "class SpacyTextAnalyzer:\n",
    "    def __init__(self):\n",
    "        # Load large model with more vectors\n",
    "        self.nlp = spacy.load('en_core_web_lg')\n",
    "    \n",
    "    def analyze_similarity(self, text1: str, text2: str) -> Dict[str, float]:\n",
    "        \"\"\"Comprehensive similarity analysis using spaCy's large model\"\"\"\n",
    "        # Process texts\n",
    "        doc1 = self.nlp(text1)\n",
    "        doc2 = self.nlp(text2)\n",
    "        \n",
    "        # Calculate different similarity metrics\n",
    "        return {\n",
    "            'doc_similarity': doc1.similarity(doc2),\n",
    "            'token_similarity': self._get_token_similarity(doc1, doc2),\n",
    "            'entity_similarity': self._get_entity_similarity(doc1, doc2)\n",
    "        }\n",
    "    \n",
    "    def _get_token_similarity(self, doc1, doc2) -> float:\n",
    "        \"\"\"Calculate average token similarity\"\"\"\n",
    "        similarities = []\n",
    "        for token1 in doc1:\n",
    "            for token2 in doc2:\n",
    "                if token1.has_vector and token2.has_vector:\n",
    "                    # Only compare meaningful tokens\n",
    "                    if not (token1.is_stop or token2.is_stop):\n",
    "                        similarities.append(token1.similarity(token2))\n",
    "        \n",
    "        return sum(similarities) / len(similarities) if similarities else 0.0\n",
    "    \n",
    "    def _get_entity_similarity(self, doc1, doc2) -> float:\n",
    "        \"\"\"Compare named entities\"\"\"\n",
    "        ents1 = set([ent.text for ent in doc1.ents])\n",
    "        ents2 = set([ent.text for ent in doc2.ents])\n",
    "        \n",
    "        if not (ents1 or ents2):\n",
    "            return 0.0\n",
    "            \n",
    "        intersection = len(ents1 & ents2)\n",
    "        union = len(ents1 | ents2)\n",
    "        return intersection / union if union > 0 else 0.0\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = SpacyTextAnalyzer()\n",
    "    \n",
    "    text1 = \"Google and Microsoft are leading AI research in Silicon Valley\"\n",
    "    text2 = \"Tech giants like Microsoft and Google are investing heavily in artificial intelligence\"\n",
    "    \n",
    "    results = analyzer.analyze_similarity(text1, text2)\n",
    "    \n",
    "    for metric, score in results.items():\n",
    "        print(f\"{metric}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some libraries and dependency (how to install - only for M3 macbook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create and activate virtual environment (recommended)\n",
    "# python3 -m venv venv\n",
    "# source venv/bin/activate\n",
    "\n",
    "# # Install dependencies for M3 Pro\n",
    "# arch -arm64 pip install python-Levenshtein\n",
    "# arch -arm64 pip install fuzzywuzzy\n",
    "# arch -arm64 pip install pandas\n",
    "# arch -arm64 pip install faker\n",
    "# arch -arm64 pip install scikit-learn\n",
    "# arch -arm64 pip install scipy\n",
    "# arch -arm64 pip install --no-cache-dir torch\n",
    "# arch -arm64 pip install sentence-transformers\n",
    "\n",
    "# # If python-Levenshtein fails, try:\n",
    "# arch -arm64 pip install python-Levenshtein-wheels\n",
    "\n",
    "# # Optional: requirements.txt\n",
    "# pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".test_install_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
